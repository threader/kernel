--- mm/rmap.c
+++ mm/rmap.c
@@ -1451,16 +1451,13 @@ bool is_vma_temporary_stack(struct vm_area_struct *vma)
 
 /**
  * try_to_unmap_anon - unmap or unlock anonymous page using the object-based
- * rmap method if @vma is NULL
+ * rmap method
  * @page: the page to unmap/unlock
  * @flags: action and flags
- * @target_vma: vma for unmapping a @page
  *
  * Find all the mappings of a page using the mapping pointer and the vma chains
  * contained in the anon_vma struct it points to.
  *
- * If @target_vma isn't NULL, this function unmap a page from the vma
- *
  * This function is only called from try_to_unmap/try_to_munlock for
  * anonymous pages.
  * When called from try_to_munlock(), the mmap_sem of the mm containing the vma
@@ -1468,19 +1465,12 @@ bool is_vma_temporary_stack(struct vm_area_struct *vma)
  * vm_flags for that VMA.  That should be OK, because that vma shouldn't be
  * 'LOCKED.
  */
-static int try_to_unmap_anon(struct page *page, enum ttu_flags flags,
-					struct vm_area_struct *target_vma)
+static int try_to_unmap_anon(struct page *page, enum ttu_flags flags)
 {
-	int ret = SWAP_AGAIN;
-	unsigned long address;
 	struct anon_vma *anon_vma;
 	pgoff_t pgoff;
 	struct anon_vma_chain *avc;
-
-	if (target_vma) {
-		address = vma_address(page, target_vma);
-		return try_to_unmap_one(page, target_vma, address, flags);
-	}
+	int ret = SWAP_AGAIN;
 
 	anon_vma = page_lock_anon_vma_read(page);
 	if (!anon_vma)
@@ -1489,6 +1479,7 @@ static int try_to_unmap_anon(struct page *page, enum ttu_flags flags,
 	pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
 	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {
 		struct vm_area_struct *vma = avc->vma;
+		unsigned long address;
 
 		/*
 		 * During exec, a temporary VMA is setup and later moved.
@@ -1516,7 +1507,6 @@ static int try_to_unmap_anon(struct page *page, enum ttu_flags flags,
  * try_to_unmap_file - unmap/unlock file page using the object-based rmap method
  * @page: the page to unmap/unlock
  * @flags: action and flags
- * @target_vma: vma for unmapping @page
  *
  * Find all the mappings of a page using the mapping pointer and the vma chains
  * contained in the address_space struct it points to.
@@ -1528,8 +1518,7 @@ static int try_to_unmap_anon(struct page *page, enum ttu_flags flags,
  * vm_flags for that VMA.  That should be OK, because that vma shouldn't be
  * 'LOCKED.
  */
-static int try_to_unmap_file(struct page *page, enum ttu_flags flags,
-				struct vm_area_struct *target_vma)
+static int try_to_unmap_file(struct page *page, enum ttu_flags flags)
 {
 	struct address_space *mapping = page->mapping;
 	pgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
@@ -1539,26 +1528,16 @@ static int try_to_unmap_file(struct page *page, enum ttu_flags flags,
 	unsigned long max_nl_cursor = 0;
 	unsigned long max_nl_size = 0;
 	unsigned int mapcount;
-	unsigned long address;
 
 	if (PageHuge(page))
 		pgoff = page->index << compound_order(page);
 
 	mutex_lock(&mapping->i_mmap_mutex);
-	if (target_vma) {
-		/* We don't handle non-linear vma on ramfs */
-		if (unlikely(!list_empty(&mapping->i_mmap_nonlinear)))
+	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
+		unsigned long address = vma_address(page, vma);
+		ret = try_to_unmap_one(page, vma, address, flags);
+		if (ret != SWAP_AGAIN || !page_mapped(page))
 			goto out;
-		address = vma_address(page, target_vma);
-		ret = try_to_unmap_one(page, target_vma, address, flags);
-		goto out;
-	} else {
-		vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
-			address = vma_address(page, vma);
-			ret = try_to_unmap_one(page, vma, address, flags);
-			if (ret != SWAP_AGAIN || !page_mapped(page))
-				goto out;
-		}
 	}
 
 	if (list_empty(&mapping->i_mmap_nonlinear))
@@ -1639,12 +1618,9 @@ static int try_to_unmap_file(struct page *page, enum ttu_flags flags,
  * try_to_unmap - try to remove all page table mappings to a page
  * @page: the page to get unmapped
  * @flags: action and flags
- * @vma : target vma for reclaim
  *
  * Tries to remove all the page table entries which are mapping this
  * page, used in the pageout path.  Caller must hold the page lock.
- * If @vma is not NULL, this function try to remove @page from only @vma
- * without peeking all mapped vma for @page.
  * Return values are:
  *
  * SWAP_SUCCESS	- we succeeded in removing all mappings
@@ -1652,8 +1628,7 @@ static int try_to_unmap_file(struct page *page, enum ttu_flags flags,
  * SWAP_FAIL	- the page is unswappable
  * SWAP_MLOCK	- page is mlocked.
  */
-int try_to_unmap(struct page *page, enum ttu_flags flags,
-				struct vm_area_struct *vma)
+int try_to_unmap(struct page *page, enum ttu_flags flags)
 {
 	int ret;
 
@@ -1661,11 +1636,11 @@ int try_to_unmap(struct page *page, enum ttu_flags flags,
 	VM_BUG_ON(!PageHuge(page) && PageTransHuge(page));
 
 	if (unlikely(PageKsm(page)))
-		ret = try_to_unmap_ksm(page, flags, vma);
+		ret = try_to_unmap_ksm(page, flags);
 	else if (PageAnon(page))
-		ret = try_to_unmap_anon(page, flags, vma);
+		ret = try_to_unmap_anon(page, flags);
 	else
-		ret = try_to_unmap_file(page, flags, vma);
+		ret = try_to_unmap_file(page, flags);
 	if (ret != SWAP_MLOCK && !page_mapped(page))
 		ret = SWAP_SUCCESS;
 	return ret;
@@ -1691,11 +1666,11 @@ int try_to_munlock(struct page *page)
 	VM_BUG_ON(!PageLocked(page) || PageLRU(page));
 
 	if (unlikely(PageKsm(page)))
-		return try_to_unmap_ksm(page, TTU_MUNLOCK, NULL);
+		return try_to_unmap_ksm(page, TTU_MUNLOCK);
 	else if (PageAnon(page))
-		return try_to_unmap_anon(page, TTU_MUNLOCK, NULL);
+		return try_to_unmap_anon(page, TTU_MUNLOCK);
 	else
-		return try_to_unmap_file(page, TTU_MUNLOCK, NULL);
+		return try_to_unmap_file(page, TTU_MUNLOCK);
 }
 
 void __put_anon_vma(struct anon_vma *anon_vma)
--- mm/rmap.c
+++ mm/rmap.c
@@ -274,6 +274,7 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 {
 	struct anon_vma_chain *avc;
 	struct anon_vma *anon_vma;
+	int error;
 
 	/* Don't bother if the parent process has no anon_vma here. */
 	if (!pvma->anon_vma)
@@ -283,8 +284,9 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 	 * First, attach the new VMA to the parent VMA's anon_vmas,
 	 * so rmap can find non-COWed pages in child processes.
 	 */
-	if (anon_vma_clone(vma, pvma))
-		return -ENOMEM;
+	error = anon_vma_clone(vma, pvma);
+	if (error)
+		return error;
 
 	/* Then add our own anon_vma. */
 	anon_vma = anon_vma_alloc();
